{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "## Model Deployment with Spark Serving \n",
                "In this example, we try to predict incomes from the *Adult Census* dataset.",
                " Then we will use Spark serving to deploy it as a realtime web service. \n",
                "First, we import needed packages:"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import sys\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import mmlspark\n"
            ],
            "metadata": {},
            "outputs": [],
            "execution_count": 3
        },
        {
            "cell_type": "markdown",
            "source": [
                "Now let's read the data and split it to train and test sets:"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "dataFilePath = \"AdultCensusIncome.csv\"\n",
                "import os, urllib\n",
                "if not os.path.isfile(dataFilePath):\n",
                "    urllib.request.urlretrieve(\"https://mmlspark.azureedge.net/datasets/\" + dataFilePath, dataFilePath)\n",
                "data = spark.createDataFrame(pd.read_csv(dataFilePath, dtype={\" hours-per-week\": np.float64}))\n",
                "data = data.select([\" education\", \" marital-status\", \" hours-per-week\", \" income\"])\n",
                "train, test = data.randomSplit([0.75, 0.25], seed=123)\n",
                "train.limit(10).toPandas()"
            ],
            "metadata": {},
            "outputs": [],
            "execution_count": 5
        },
        {
            "cell_type": "markdown",
            "source": [
                "`TrainClassifier` can be used to initialize and fit a model, it wraps SparkML classifiers.\n",
                "You can use `help(mmlspark.TrainClassifier)` to view the different parameters.\n",
                "\nNote that it implicitly converts the data into the format expected by the algorithm. More specifically it:\n",
                " tokenizes, hashes strings, one-hot encodes categorical variables, assembles the features into a vector\n",
                "etc.  The parameter `numFeatures` controls the number of hashed features."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "from mmlspark import TrainClassifier\n",
                "from pyspark.ml.classification import LogisticRegression\n",
                "model = TrainClassifier(model=LogisticRegression(), labelCol=\" income\", numFeatures=256).fit(train)"
            ],
            "metadata": {},
            "outputs": [],
            "execution_count": 7
        },
        {
            "cell_type": "markdown",
            "source": [
                "After the model is trained, we score it against the test dataset and view metrics."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "from mmlspark import ComputeModelStatistics, TrainedClassifierModel\n",
                "prediction = model.transform(test)\n",
                "prediction.printSchema()"
            ],
            "metadata": {},
            "outputs": [],
            "execution_count": 9
        },
        {
            "cell_type": "code",
            "source": [
                "metrics = ComputeModelStatistics().transform(prediction)\n",
                "metrics.limit(10).toPandas()"
            ],
            "metadata": {},
            "outputs": [],
            "execution_count": 10
        },
        {
            "cell_type": "markdown",
            "source": [
            	"First, we will define the webservice input/output.\n",
            	"For more information, you can visit the [documentation for Spark Serving](https://github.com/Azure/mmlspark/blob/master/docs/mmlspark-serving.md)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "from pyspark.sql.functions import col, from_json\n",
                "from pyspark.sql.types import *\n",
                "import uuid\n\n",
                "serving_inputs = spark.readStream.server() \\\n",
                "    .address(\"localhost\", 8888, \"my_api\") \\\n",
                "    .load()\\\n",
                "    .withColumn(\"variables\", from_json(col(\"value\"), test.schema))\\\n",
                "    .select(\"id\",\"variables.*\")\n\n",
                "serving_outputs = model.transform(serving_inputs) \\\n",
                "  .withColumn(\"scored_labels\", col(\"scored_labels\").cast(\"string\"))\n\n",
                "server = serving_outputs.writeStream \\\n",
                "    .server() \\\n",
                "    .option(\"name\", \"my_api\") \\\n",
                "    .queryName(\"my_query\") \\\n",
                "    .option(\"replyCol\", \"scored_labels\") \\\n",
                "    .option(\"checkpointLocation\", \"checkpoints-{}\".format(uuid.uuid1())) \\\n",
                "    .start()\n"
            ],
            "metadata": {},
            "outputs": [],
            "execution_count": 15
        },
        {
            "cell_type": "markdown",
            "source": [
                "Test the webservice"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import requests\n",
                "data = u'{\" education\":\" 10th\",\" marital-status\":\" Divorced\",\" hours-per-week\":40.0,\" income\":\" <=50K\"}'\n",
                "r = requests.post(data=data, url=\"http://localhost:8888/my_api\")\n",
                "print(\"Response {}\".format(r.text))"
            ],
            "metadata": {},
            "outputs": [],
            "execution_count": 17
        },
        {
            "cell_type": "code",
            "source": [
                "import time\n",
                "time.sleep(20) # wait for server to finish setting up (just to be safe)\n",
                "server.stop()"
            ],
            "metadata": {},
            "outputs": [],
            "execution_count": 17
        }
    ],
    "metadata": {
        "language_info": {
            "codemirror_mode": {
                "name": "python",
                "version": "3"
            },
            "mimetype": "text/x-python",
            "name": "pyspark3",
            "pygments_lexer": "python3"
        },
        "name": "108 - Model Deployment with Spark Serving",
        "notebookId": 496516146626304,
        "kernelspec": {
            "display_name": "PySpark3",
            "language": "",
            "name": "pyspark3kernel"
        },
        "anaconda-cloud": {}
    },
    "nbformat": 4,
    "nbformat_minor": 0
}

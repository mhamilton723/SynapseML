{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "## Interpretability - Tabular SHAP explainer\n\nIn this example, we use Kernel SHAP to explain a tabular classification model built from the Adults Census dataset.\n\nFirst we import the packages and define some UDFs we will need later."
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "title": "",
                    "showTitle": false,
                    "inputWidgets": {},
                    "nuid": "4a463c67-7543-42d2-a116-e70e8451b09b"
                }
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import pyspark\nfrom mmlspark.explainers import *\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nimport pandas as pd\n\nvec_access = udf(lambda v, i: float(v[i]), FloatType())\nvec2array = udf(lambda vec: vec.toArray().tolist(), ArrayType(FloatType()))"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "title": "",
                    "showTitle": false,
                    "inputWidgets": {},
                    "nuid": "bf0fdfc2-97b2-48e4-b3d9-794b0cb3da67"
                }
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "metadata": {
                        "application/vnd.databricks.v1+output": {
                            "data": "",
                            "errorSummary": "",
                            "metadata": {},
                            "errorTraceType": null,
                            "type": "ipynbError",
                            "arguments": {}
                        }
                    },
                    "output_type": "display_data",
                    "data": {
                        "text/html": [
                            "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
                        ]
                    }
                }
            ],
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                "Now let's read the data and train a simple binary classification model."
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "title": "",
                    "showTitle": false,
                    "inputWidgets": {},
                    "nuid": "ae47e1f9-0672-47ed-94de-10970e1b14b5"
                }
            }
        },
        {
            "cell_type": "code",
            "source": [
                "df = spark.read.parquet(\"wasbs://publicwasb@mmlspark.blob.core.windows.net/AdultCensusIncome.parquet\")\n\nlabelIndexer = StringIndexer(inputCol=\"income\", outputCol=\"label\", stringOrderType=\"alphabetAsc\").fit(df)\nprint(\"Label index assigment: \" + str(set(zip(labelIndexer.labels, [0, 1]))))\n\ntraining = labelIndexer.transform(df)\ndisplay(training)\ncategorical_features = [\n    \"workclass\",\n    \"education\",\n    \"marital-status\",\n    \"occupation\",\n    \"relationship\",\n    \"race\",\n    \"sex\",\n    \"native-country\",\n]\ncategorical_features_idx = [col + \"_idx\" for col in categorical_features]\ncategorical_features_enc = [col + \"_enc\" for col in categorical_features]\nnumeric_features = [\"age\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n\nstrIndexer = StringIndexer(inputCols=categorical_features, outputCols=categorical_features_idx)\nonehotEnc = OneHotEncoder(inputCols=categorical_features_idx, outputCols=categorical_features_enc)\nvectAssem = VectorAssembler(inputCols=categorical_features_enc + numeric_features, outputCol=\"features\")\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", weightCol=\"fnlwgt\")\npipeline = Pipeline(stages=[strIndexer, onehotEnc, vectAssem, lr])\nmodel = pipeline.fit(training)"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "title": "",
                    "showTitle": false,
                    "inputWidgets": {},
                    "nuid": "58807448-d8e0-4818-adc8-27536d561fb3"
                }
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "metadata": {
                        "application/vnd.databricks.v1+output": {
                            "data": "",
                            "errorSummary": "",
                            "metadata": {},
                            "errorTraceType": null,
                            "type": "ipynbError",
                            "arguments": {}
                        }
                    },
                    "output_type": "display_data",
                    "data": {
                        "text/html": [
                            "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
                        ]
                    }
                }
            ],
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                "After the model is trained, we randomly select some observations to be explained."
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "title": "",
                    "showTitle": false,
                    "inputWidgets": {},
                    "nuid": "f617f9a4-7e67-43f8-8fa9-92680b635b3d"
                }
            }
        },
        {
            "cell_type": "code",
            "source": [
                "explain_instances = model.transform(training).orderBy(rand()).limit(5).repartition(200).cache()\ndisplay(explain_instances)"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "title": "",
                    "showTitle": false,
                    "inputWidgets": {},
                    "nuid": "f55757a6-6204-4f64-a91e-65bfbacf62bc"
                }
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "metadata": {
                        "application/vnd.databricks.v1+output": {
                            "data": "",
                            "errorSummary": "",
                            "metadata": {},
                            "errorTraceType": null,
                            "type": "ipynbError",
                            "arguments": {}
                        }
                    },
                    "output_type": "display_data",
                    "data": {
                        "text/html": [
                            "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
                        ]
                    }
                }
            ],
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                "We create a TabularSHAP explainer, set the input columns to all the features the model takes, specify the model and the target output column we are trying to explain. In this case, we are trying to explain the \"probability\" output which is a vector of length 2, and we are only looking at class 1 probability. Specify targetClasses to `[0, 1]` if you want to explain class 0 and 1 probability at the same time. Finally we sample 100 rows from the training data for background data, which is used for integrating out features in Kernel SHAP."
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "title": "",
                    "showTitle": false,
                    "inputWidgets": {},
                    "nuid": "48a0c8ee-8e36-4bd3-9a04-eded6d2c8894"
                }
            }
        },
        {
            "cell_type": "code",
            "source": [
                "shap = TabularSHAP(\n    inputCols=categorical_features + numeric_features,\n    outputCol=\"shapValues\",\n    numSamples=5000,\n    model=model,\n    targetCol=\"probability\",\n    targetClasses=[1],\n    backgroundData=training.orderBy(rand()).limit(100).cache(),\n)\n\nshap_df = shap.transform(explain_instances)\n"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "title": "",
                    "showTitle": false,
                    "inputWidgets": {},
                    "nuid": "7e097552-e617-4e1c-a085-b66eca5bcb69"
                }
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "metadata": {
                        "application/vnd.databricks.v1+output": {
                            "data": "",
                            "errorSummary": "",
                            "metadata": {},
                            "errorTraceType": null,
                            "type": "ipynbError",
                            "arguments": {}
                        }
                    },
                    "output_type": "display_data",
                    "data": {
                        "text/html": [
                            "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
                        ]
                    }
                }
            ],
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                "Once we have the resulting dataframe, we extract the class 1 probability of the model output, the SHAP values for the target class, the original features and the true label. Then we convert it to a pandas dataframe for visisualization.\nFor each observation, the first element in the SHAP values vector is the base value (the mean output of the background dataset), and each of the following element is the SHAP values for each feature."
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "title": "",
                    "showTitle": false,
                    "inputWidgets": {},
                    "nuid": "6933b52b-7d46-4210-810a-f984b76dd4a2"
                }
            }
        },
        {
            "cell_type": "code",
            "source": [
                "shaps = (\n    shap_df.withColumn(\"probability\", vec_access(col(\"probability\"), lit(1)))\n    .withColumn(\"shapValues\", vec2array(col(\"shapValues\").getItem(0)))\n    .select([\"shapValues\", \"probability\", \"label\"] + categorical_features + numeric_features)\n)\n\nshaps_local = shaps.toPandas()\nshaps_local.sort_values(\"probability\", ascending=False, inplace=True, ignore_index=True)\npd.set_option(\"display.max_colwidth\", None)\nshaps_local"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "title": "",
                    "showTitle": false,
                    "inputWidgets": {},
                    "nuid": "05e01f98-e44c-46c9-a8ae-26ba892f85b3"
                }
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "metadata": {
                        "application/vnd.databricks.v1+output": {
                            "data": "",
                            "errorSummary": "",
                            "metadata": {},
                            "errorTraceType": null,
                            "type": "ipynbError",
                            "arguments": {}
                        }
                    },
                    "output_type": "display_data",
                    "data": {
                        "text/html": [
                            "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
                        ]
                    }
                }
            ],
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                "We use plotly subplot to visualize the SHAP values."
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "title": "",
                    "showTitle": false,
                    "inputWidgets": {},
                    "nuid": "f9317a27-900a-4d1d-9e9f-9fe906eae75c"
                }
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport pandas as pd\n\nfeatures = categorical_features + numeric_features\nfeatures_with_base = [\"Base\"] + features\n\nrows = shaps_local.shape[0]\n\nfig = make_subplots(\n    rows=rows,\n    cols=1,\n    subplot_titles=\"Probability: \" + shaps_local[\"probability\"].apply(\"{:.2%}\".format) + \"; Label: \" + shaps_local[\"label\"].astype(str),\n)\n\nfor index, row in shaps_local.iterrows():\n    feature_values = [0] + [row[feature] for feature in features]\n    shap_values = row[\"shapValues\"]\n    list_of_tuples = list(zip(features_with_base, feature_values, shap_values))\n    shap_pdf = pd.DataFrame(list_of_tuples, columns=[\"name\", \"value\", \"shap\"])\n    fig.add_trace(\n        go.Bar(x=shap_pdf[\"name\"], y=shap_pdf[\"shap\"], hovertext=\"value: \" + shap_pdf[\"value\"].astype(str)),\n        row=index + 1,\n        col=1,\n    )\n\nfig.update_yaxes(range=[-1, 1], fixedrange=True, zerolinecolor=\"black\")\nfig.update_xaxes(type=\"category\", tickangle=45, fixedrange=True)\nfig.update_layout(height=400 * rows, title_text=\"SHAP explanations\")\nfig.show()\n"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "title": "",
                    "showTitle": false,
                    "inputWidgets": {},
                    "nuid": "c9b4c03e-eac8-4314-a6c2-0a451525e6a4"
                }
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "metadata": {
                        "application/vnd.databricks.v1+output": {
                            "data": "",
                            "errorSummary": "",
                            "metadata": {},
                            "errorTraceType": null,
                            "type": "ipynbError",
                            "arguments": {}
                        }
                    },
                    "output_type": "display_data",
                    "data": {
                        "text/html": [
                            "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
                        ]
                    }
                }
            ],
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                "Your results will look like:\n\n<img src=\"https://mmlspark.blob.core.windows.net/graphics/explainers/tabular-shap.png\" style=\"float: right;\"/>"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "title": "",
                    "showTitle": false,
                    "inputWidgets": {},
                    "nuid": "8f22fceb-0fc0-4a86-a0ca-2a7b47b4795a"
                }
            }
        }
    ],
    "metadata": {
        "application/vnd.databricks.v1+notebook": {
            "notebookName": "Interpretability - Tabular SHAP explainer",
            "dashboards": [],
            "notebookMetadata": {
                "pythonIndentUnit": 2
            },
            "language": "python",
            "widgets": {},
            "notebookOrigID": 4343954975413564
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}